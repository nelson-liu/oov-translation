# oov-translation

Models and code for translating out of vocabulary words.

The code in this repo is meant to be hooked up to a blackbox MT system for
last-minute translation (post-editing) of out-of-vocabulary tokens.

## Table of Contents

- [Installation](#installation)
  * [Installing this repo](#installing-this-repo)
  * [Installing necessary requirements](#installing-necessary-requirements)
  * [Installing PyTorch and TorchText](#installing-pytorch-and-torchtext)
- [Quick Start](#quick-start)
  * [Training Models](#training-models)
  * [Loading Models and {Predicting / Resuming Training}](#loading-models-and-predicting--resuming-training)
- [Solver Public API](#solver-public-api)
- [Tips and Tricks for Experiment Config Files](#tips-and-tricks-for-experiment-config-files)
- [Seq2Seq Hyperparameter Search](#seq2seq-hyperparameter-search)
- [Contributing](#contributing)
- [Contact](#contact)


## Installation

This project is being developed in Python 3.6, but is tested on **Python 2.7 and
Python 3.6**. I've tried my best to make it compatible with non-Python 3.6
versions, but I make no guarantees. In particular, Unicode handling in Python
2.7 might not be perfect. Please raise an issue if you find that this is the
case!

### Installing this repo

Clone this repo, and then run:

```bash
git submodule update --init --recursive
```

to pull all the submodules as well.


### Installing necessary requirements

The package requirements are in [`requirements.txt`](./requirements.txt).

To install the requirements, run:

```bash
pip install -r requirements.txt
```

### Installing PyTorch and TorchText

Some of the models also require PyTorch (*version 0.2*).

- The PyTorch install command is system-dependent,
  consult
  [their documentation](https://github.com/pytorch/pytorch#installation) for
  more info.
- To install on Linux running Python 3.6, use: `conda install pytorch=0.2.0 cuda80 -c soumith`
  
## Quick Start

For any script, running `python <script_path.py> -h` will print help and usage instructions.

To train a model (a "solver", in the technical jargon of this project) to
translate OOV words,
use [`./scripts/run/run_oov_solver.py`](./scripts/run/run_oov_solver.py). Given an
experiment JSON configuration and/or command-line arguments, this script can
train, save, load a model, and make predictions on external data files.

### Training Models

To train a character-level sequence to sequence model for OOV translation (say,
for Amharic to English), you have to:

1. Modify the experiment file
  at
  [`./experiment_configs/seq2seq/char/amh-eng_best_char_seq2seq_oov_translation_experiment.json`](./experiment_configs/seq2seq/char/amh-eng_best_char_seq2seq_oov_translation_experiment.json) to
  point to the paths of your train and validation data, as well as perhaps edit
  the directories to save model checkpoints and write Tensorboard log summaries.
  In addition, you can customize almost any aspect of the sequence to sequence
  model simply by changing the values in this JSON configuration file.
  
2. Move your terminal to the project root and run:

```bash
python scripts/run/run_oov_solver.py \
    --config_path=./experiment_configs/seq2seq/char/amh-eng_best_char_seq2seq_oov_translation_experiment.json
```

If all went well, the model will begin training, writing checkpoints and
Tensorboard summaries along the way to your folders of choice.

Note that the same steps can be used for *any* model that conforms to the
`BaseSolver` API (most of them). For example, to train an `EditDistanceSolver`,
just pass the `run_oov_solver.py` script the path to an `EditDistanceSolver`
config (e.g., any of the ones at
`./experiment_configs/similarity/edit_distance`).

### Loading Models and {Predicting / Resuming Training}

To load a model from a serialized version generated by
by [`./scripts/run/run_oov_solver.py`](./scripts/run/run_oov_solver.py) and use it to
make predictions on an `eval_file`, run the following:

```bash
python scripts/run/run_oov_solver.py \
    --predict_from_path=<model_path> \
    --eval_path=<eval_file_path>
```

The model should go off and make predictions, and will write its guesses to a
subdirectory of the `eval_path` folder called `guess` in a file with its
`run_id`. Note that the seq2seq code write models with the extension `.ckpt`,
while the rest use the extension `.pkl`.

To load a *seq2seq* model from a saved checkpoint and resume training on the
same dataset as specified in the original model configuration, run the
following:

```bash
python scripts/run/run_oov_solver.py \
    --train_from_path=<checkpoint_path>
```

This only works on seq2seq models, for now; training the other models is fast
enough that it usually isn't something that has to be interrupted and restarted.

You may also want to check out the [tutorial](./docs/tutorial/tutorial.md) to
get some more details about how to use this repository.

## Solver Public API

The various [solvers](./oov/models/) all implement (at minimum) a common public
API, which is outlined in [`base_solver.py`](./oov/models/base_solver.py). If a
model subclasses `BaseSolver`, you can assume that it's a proper solver and will
thus work with [`./scripts/run/run_oov_solver.py`](./scripts/run/run_oov_solver.py).
Adding a new solver is as simple as subclassing `BaseSolver` and implementing
the appropriate public functions.

## Tips and Tricks for Experiment Config Files

The experiment config files are pretty powerful, letting you customize almost
any aspect of any solver to your liking. In the most basic sense, you can think
of the config files as merely providing a convenient place for you to store
command-line arguments you would otherwise pass
to [`./scripts/run/run_oov_solver.py`](./scripts/run/run_oov_solver.py). It also
enables you to quickly rerun something with the same set of configurations or
minor tweaks.

As a result, you can pass in values via command line flags
to [`./scripts/run/run_oov_solver.py`](./scripts/run/run_oov_solver.py) --- if the
value is already set in the configuration file, the one in the command line flag
takes precedence.

For more info about what sort of things you can customize with config files, run
`python ./scripts/run/run_oov_solver.py -h` or poke around some of the experiment
config files already in this repo.

## Seq2Seq Hyperparameter Search

To run hyperparameter search with a seq2seq model,
use [`./scripts/run/run_seq2seq_hpsearch.py`](./scripts/run_seq2seq_hpsearch.py).

This script takes a variable number of random hyperparameter configurations
(from a specified hyperparameter grid) and trains a model with each of them over
a variable number of GPUs. It's possible to control the GPUs to use, as well as
how many models to put on each GPU, for greater efficiency.

To run, you can use (for example):

```bash
python scripts/run/run_seq2seq_hpsearch.py \
    --config_path=./hpsearch_configs/seq2seq/char/seq2seq_char_urd-eng_hpsearch.json
```

**Note:** the hyperparameter search script works by running a bunch of child
processes --- make sure your system has enough memory to support all of the
models running concurrently. In addition, it's possible that killing the
hyperparameter script would leave zombified children processes still running.
It's advised to use your favorite process manager (e.g. `ps` or `htop`) to
verify that none of the launched train scripts are still running if you ever
have to kill the hyperparameter training script.
